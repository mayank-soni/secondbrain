{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.data.telegram import TelegramReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = TelegramReader()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw Telegram JSON file is read as a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AIAP12</td>\n",
       "      <td>private_group</td>\n",
       "      <td>828903753</td>\n",
       "      <td>{'id': 136009, 'type': 'service', 'date': '202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AIAP12</td>\n",
       "      <td>private_group</td>\n",
       "      <td>828903753</td>\n",
       "      <td>{'id': 136010, 'type': 'service', 'date': '202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AIAP12</td>\n",
       "      <td>private_group</td>\n",
       "      <td>828903753</td>\n",
       "      <td>{'id': 136011, 'type': 'service', 'date': '202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AIAP12</td>\n",
       "      <td>private_group</td>\n",
       "      <td>828903753</td>\n",
       "      <td>{'id': 136013, 'type': 'service', 'date': '202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AIAP12</td>\n",
       "      <td>private_group</td>\n",
       "      <td>828903753</td>\n",
       "      <td>{'id': 136014, 'type': 'service', 'date': '202...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name           type         id   \n",
       "0  AIAP12  private_group  828903753  \\\n",
       "1  AIAP12  private_group  828903753   \n",
       "2  AIAP12  private_group  828903753   \n",
       "3  AIAP12  private_group  828903753   \n",
       "4  AIAP12  private_group  828903753   \n",
       "\n",
       "                                            messages  \n",
       "0  {'id': 136009, 'type': 'service', 'date': '202...  \n",
       "1  {'id': 136010, 'type': 'service', 'date': '202...  \n",
       "2  {'id': 136011, 'type': 'service', 'date': '202...  \n",
       "3  {'id': 136013, 'type': 'service', 'date': '202...  \n",
       "4  {'id': 136014, 'type': 'service', 'date': '202...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messages column contains a dictionary with various pieces of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action',\n",
       " 'actor',\n",
       " 'actor_id',\n",
       " 'date',\n",
       " 'date_unixtime',\n",
       " 'edited',\n",
       " 'edited_unixtime',\n",
       " 'file',\n",
       " 'forwarded_from',\n",
       " 'from',\n",
       " 'from_id',\n",
       " 'height',\n",
       " 'id',\n",
       " 'inviter',\n",
       " 'members',\n",
       " 'message_id',\n",
       " 'mime_type',\n",
       " 'photo',\n",
       " 'reply_to_message_id',\n",
       " 'text',\n",
       " 'text_entities',\n",
       " 'thumbnail',\n",
       " 'type',\n",
       " 'via_bot',\n",
       " 'width'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dict_keys(series: pd.Series, is_list=False) -> set:\n",
    "    \"\"\"Get set of unique keys from a Series of dictionaries\n",
    "    (or a series of lists of dictionaries).\"\"\"\n",
    "    keys = set()\n",
    "    for row in series:\n",
    "        # Added to handle nested lists of dictionaries (e.g. text_entities)\n",
    "        if is_list:\n",
    "            for item in row:\n",
    "                keys.update(item.keys())\n",
    "        else:\n",
    "            keys.update(row.keys())\n",
    "    return keys\n",
    "\n",
    "\n",
    "get_dict_keys(reader.df[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 136143, 'type': 'message', 'date': '2023-03-03T15:32:32', 'date_unixtime': '1677828752', 'from': 'paul', 'from_id': 'user1139710027', 'text': 'Thx', 'text_entities': [{'type': 'plain', 'text': 'Thx'}]}\n",
      "{'id': 136047, 'type': 'message', 'date': '2023-02-23T20:32:20', 'date_unixtime': '1677155540', 'from': 'Mayank Soni', 'from_id': 'user565286338', 'text': [{'type': 'bot_command', 'text': '/in'}], 'text_entities': [{'type': 'bot_command', 'text': '/in'}]}\n",
      "{'id': 136200, 'type': 'message', 'date': '2023-03-07T13:31:57', 'date_unixtime': '1678167117', 'from': 'Walter Teng', 'from_id': 'user299206190', 'text': 'say too easy lol', 'text_entities': [{'type': 'plain', 'text': 'say too easy lol'}]}\n",
      "{'id': 136125, 'type': 'message', 'date': '2023-03-02T12:38:36', 'date_unixtime': '1677731916', 'from': 'Yan Liong Tan', 'from_id': 'user1904912399', 'text': 'lower cafe in guild house', 'text_entities': [{'type': 'plain', 'text': 'lower cafe in guild house'}]}\n",
      "{'id': 136279, 'type': 'message', 'date': '2023-03-16T11:07:18', 'date_unixtime': '1678936038', 'from': 'Yan Liong Tan', 'from_id': 'user1904912399', 'text': [{'type': 'link', 'text': 'https://twitter.com/grdecter/status/1635996439171018757?s=12&t=oTzRSyzb769T8nS7_siz2Q'}], 'text_entities': [{'type': 'link', 'text': 'https://twitter.com/grdecter/status/1635996439171018757?s=12&t=oTzRSyzb769T8nS7_siz2Q'}]}\n"
     ]
    }
   ],
   "source": [
    "for row in reader.df[\"messages\"].sample(5):\n",
    "    print(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the text and text_entities fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our master ng\n",
      "[{'type': 'plain', 'text': 'our master ng'}]\n",
      "----\n",
      "Where u ah\n",
      "[{'type': 'plain', 'text': 'Where u ah'}]\n",
      "----\n",
      "i think lately i've become a stanford salesman ðŸ˜‚\n",
      "[{'type': 'plain', 'text': \"i think lately i've become a stanford salesman ðŸ˜‚\"}]\n",
      "----\n",
      "What we doing?\n",
      "[{'type': 'plain', 'text': 'What we doing?'}]\n",
      "----\n",
      "['liong said this was helpful for him \\n', {'type': 'link', 'text': 'https://keras.io/examples/vision/visualizing_what_convnets_learn/'}]\n",
      "[{'type': 'plain', 'text': 'liong said this was helpful for him \\n'}, {'type': 'link', 'text': 'https://keras.io/examples/vision/visualizing_what_convnets_learn/'}]\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "def text_compare_text_entities(messages: pd.Series):\n",
    "    for row in messages:\n",
    "        print(row[\"text\"])\n",
    "        print(row[\"text_entities\"])\n",
    "        print(\"----\")\n",
    "\n",
    "\n",
    "text_compare_text_entities(reader.df[\"messages\"].sample(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like text is the same as text_entities, except that it represents text_entities of type 'plain' as simple plain text. Let's confirm\n",
    "\n",
    "Ok, so another difference is that empty messages are represented as an empty string in text and an empty list in text_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bot_command', 'link', 'mention_name'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_equals_text_entities(row: dict) -> bool:\n",
    "    \"\"\"Check if text and text_entities are the same.\"\"\"\n",
    "    return row[\"text\"] == row[\"text_entities\"]\n",
    "\n",
    "\n",
    "# Boolean filter for rows where text and text_entities are the same\n",
    "same = reader.df[\"messages\"].apply(text_equals_text_entities)\n",
    "\n",
    "\n",
    "def get_text_entity_types(messages: pd.Series, is_text_entities: bool = False) -> set:\n",
    "    \"\"\"Get set of values of 'type' in text entities.\n",
    "    If is_text_entities is True, then text_entities have already been extracted from messages and are directly passed in\n",
    "    \"\"\"\n",
    "    types = set()\n",
    "    for row in messages:\n",
    "        # Added to handle if text_entities already extracted before being passed in\n",
    "        row = row if is_text_entities else row[\"text_entities\"]\n",
    "        for item in row:\n",
    "            types.add(item.get(\"type\"))\n",
    "    return types\n",
    "\n",
    "\n",
    "# Check if plain exists in messages where text_entities and text are the same.\n",
    "get_text_entity_types(reader.df[\"messages\"][same])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'plain'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text_entity_type_exists_in_all(messages: pd.Series) -> set:\n",
    "    \"\"\"Get set of values of 'type' that exist in all messages.\"\"\"\n",
    "    types = get_text_entity_types(messages)\n",
    "    for row in messages:\n",
    "        individual_types = get_text_entity_types(pd.Series([row]))\n",
    "        # Remove empty sets (i.e. messages with no text_entities)\n",
    "        if len(individual_types) > 0:\n",
    "            types.intersection_update(individual_types)\n",
    "    return types\n",
    "\n",
    "\n",
    "# Check if plain is the only text entity type that exists in all messages where text_entities and text are not the same.\n",
    "get_text_entity_type_exists_in_all(reader.df[\"messages\"][~same])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use text entities as it has a more consistent format (all lists of dictionaries). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm types of text_entities are the same for all messages\n",
    "for row in reader.df[\"messages\"]:\n",
    "    assert type(row[\"text_entities\"]) == list\n",
    "    for item in row[\"text_entities\"]:\n",
    "        assert type(item) == dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of text entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286         [{'type': 'plain', 'text': 'our master ng'}]\n",
       "136    [{'type': 'plain', 'text': 'if wan to kill the...\n",
       "98     [{'type': 'plain', 'text': 'i set the interpre...\n",
       "257    [{'type': 'plain', 'text': 'elon musk sign so ...\n",
       "14                                                    []\n",
       "Name: text_entities, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.text_entities.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the lists have a single element, but some have 0 and some have > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text_entities\n",
       "0      40\n",
       "1     242\n",
       "2      41\n",
       "3      14\n",
       "4       3\n",
       "5       1\n",
       "6       3\n",
       "7       3\n",
       "9       2\n",
       "10      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.text_entities.apply(len).value_counts().sort_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text entities with 0 items"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are either people joining/leaving the group or photos or files i.e. there's no text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'service', 'actor': 'Mayank Soni', 'actor_id': 'user565286338', 'action': 'join_group_by_link', 'inviter': 'Group', 'text': '', 'text_entities': []}\n",
      "{'type': 'service', 'actor': 'Loi Xue Zheng', 'actor_id': 'user234580032', 'action': 'join_group_by_link', 'inviter': 'Group', 'text': '', 'text_entities': []}\n",
      "{'type': 'service', 'actor': 'Shu Ying', 'actor_id': 'user5362962200', 'action': 'join_group_by_link', 'inviter': 'Group', 'text': '', 'text_entities': []}\n",
      "{'type': 'service', 'actor': 'Walter Teng', 'actor_id': 'user299206190', 'action': 'join_group_by_link', 'inviter': 'Group', 'text': '', 'text_entities': []}\n",
      "{'type': 'service', 'actor': 'Marvin Ng', 'actor_id': 'user1371801874', 'action': 'join_group_by_link', 'inviter': 'Group', 'text': '', 'text_entities': []}\n",
      "{'type': 'service', 'actor': 'Zhi Qiang Quek', 'actor_id': 'user752030617', 'action': 'join_group_by_link', 'inviter': 'Group', 'text': '', 'text_entities': []}\n",
      "{'type': 'service', 'actor': 'Bryan AIAP', 'actor_id': 'user177304688', 'action': 'join_group_by_link', 'inviter': 'Group', 'text': '', 'text_entities': []}\n",
      "{'type': 'service', 'actor': 'Alvin', 'actor_id': 'user241955311', 'action': 'join_group_by_link', 'inviter': 'Group', 'text': '', 'text_entities': []}\n",
      "{'type': 'service', 'actor': 'JF Koh', 'actor_id': 'user1268174559', 'action': 'join_group_by_link', 'inviter': 'Group', 'text': '', 'text_entities': []}\n",
      "{'type': 'service', 'actor': 'Jia Hao Khaw', 'actor_id': 'user41568057', 'action': 'join_group_by_link', 'inviter': 'Group', 'text': '', 'text_entities': []}\n",
      "{'type': 'service', 'actor': 'Loi Xue Zheng', 'actor_id': 'user234580032', 'action': 'invite_members', 'members': ['Zhi Xuan'], 'text': '', 'text_entities': []}\n",
      "{'type': 'service', 'actor': 'Mayank Soni', 'actor_id': 'user565286338', 'action': 'invite_members', 'members': ['WhosInBot'], 'text': '', 'text_entities': []}\n",
      "{'type': 'service', 'actor': 'Mayank Soni', 'actor_id': 'user565286338', 'action': 'remove_members', 'members': ['WhosInBot'], 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'edited': '2023-02-28T16:11:57', 'edited_unixtime': '1677571917', 'from': 'Hanafi Haffidz', 'from_id': 'user44489326', 'photo': 'photos/photo_1@28-02-2023_14-13-05.jpg', 'width': 547, 'height': 542, 'text': '', 'text_entities': []}\n",
      "{'type': 'service', 'actor': 'paul', 'actor_id': 'user1139710027', 'action': 'invite_members', 'members': ['Winnie Wan'], 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'from': 'Hanafi Haffidz', 'from_id': 'user44489326', 'photo': 'photos/photo_3@28-02-2023_21-50-35.jpg', 'width': 1280, 'height': 230, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'edited': '2023-03-01T12:33:51', 'edited_unixtime': '1677645231', 'from': 'paul', 'from_id': 'user1139710027', 'file': 'files/dockube_related_books.zip', 'mime_type': 'application/zip', 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'from': 'Yan Liong Tan', 'from_id': 'user1904912399', 'photo': 'photos/photo_5@01-03-2023_12-32-48.jpg', 'width': 591, 'height': 1280, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'from': 'Yan Liong Tan', 'from_id': 'user1904912399', 'photo': 'photos/photo_6@02-03-2023_11-34-57.jpg', 'width': 1280, 'height': 960, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'from': 'Yan Liong Tan', 'from_id': 'user1904912399', 'photo': 'photos/photo_8@02-03-2023_12-38-24.jpg', 'width': 960, 'height': 1280, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'from': 'Yan Liong Tan', 'from_id': 'user1904912399', 'photo': 'photos/photo_9@02-03-2023_12-38-24.jpg', 'width': 960, 'height': 1280, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'from': 'Yan Liong Tan', 'from_id': 'user1904912399', 'photo': 'photos/photo_11@02-03-2023_12-39-11.jpg', 'width': 960, 'height': 1280, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'from': 'Yan Liong Tan', 'from_id': 'user1904912399', 'photo': 'photos/photo_12@02-03-2023_12-39-11.jpg', 'width': 960, 'height': 1280, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'from': 'paul', 'from_id': 'user1139710027', 'photo': 'photos/photo_15@03-03-2023_16-55-04.jpg', 'width': 1280, 'height': 237, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'edited': '2023-03-05T09:24:30', 'edited_unixtime': '1677979470', 'from': 'Alvin', 'from_id': 'user241955311', 'photo': 'photos/photo_16@05-03-2023_09-14-15.jpg', 'width': 1080, 'height': 387, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'edited': '2023-03-06T12:46:19', 'edited_unixtime': '1678077979', 'from': 'Shaun', 'from_id': 'user158330012', 'forwarded_from': 'Hanafi Haffidz', 'file': 'files/gcp-sa.json', 'mime_type': 'application/json', 'text': '', 'text_entities': []}\n",
      "{'type': 'service', 'actor': 'Ben', 'actor_id': 'user5110707292', 'action': 'join_group_by_link', 'inviter': 'Group', 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'from': 'Yan Liong Tan', 'from_id': 'user1904912399', 'photo': 'photos/photo_17@10-03-2023_11-57-59.jpg', 'width': 1280, 'height': 1084, 'text': '', 'text_entities': []}\n",
      "{'type': 'service', 'actor': 'Mayank Soni', 'actor_id': 'user565286338', 'action': 'pin_message', 'message_id': 136203, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'from': 'Hanafi Haffidz', 'from_id': 'user44489326', 'reply_to_message_id': 136250, 'photo': 'photos/photo_18@15-03-2023_09-33-10.jpg', 'width': 749, 'height': 448, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'from': 'Marvin Ng', 'from_id': 'user1371801874', 'photo': 'photos/photo_19@15-03-2023_09-38-36.jpg', 'width': 555, 'height': 736, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'from': 'Wayne Lau', 'from_id': 'user42283285', 'photo': 'photos/photo_21@15-03-2023_16-44-02.jpg', 'width': 614, 'height': 577, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'from': 'Yan Liong Tan', 'from_id': 'user1904912399', 'photo': 'photos/photo_22@15-03-2023_18-01-45.jpg', 'width': 591, 'height': 1280, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'from': 'Hanafi Haffidz', 'from_id': 'user44489326', 'photo': 'photos/photo_26@30-03-2023_10-29-32.jpg', 'width': 1280, 'height': 210, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'edited': '2023-04-03T14:15:06', 'edited_unixtime': '1680502506', 'from': 'Jong Ching', 'from_id': 'user445787613', 'photo': 'photos/photo_28@03-04-2023_14-14-48.jpg', 'width': 720, 'height': 1280, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'edited': '2023-04-04T21:24:13', 'edited_unixtime': '1680614653', 'from': 'Bryan AIAP', 'from_id': 'user177304688', 'photo': 'photos/photo_29@04-04-2023_19-35-05.jpg', 'width': 570, 'height': 476, 'text': '', 'text_entities': []}\n",
      "{'type': 'service', 'actor': 'Wayne Lau', 'actor_id': 'user42283285', 'action': 'edit_group_photo', 'photo': 'photos/photo_30@11-04-2023_12-34-29.jpg', 'width': 640, 'height': 640, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'from': 'Alvin', 'from_id': 'user241955311', 'photo': 'photos/photo_31@11-04-2023_17-54-13.jpg', 'width': 659, 'height': 1280, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'from': 'Zhi Xuan', 'from_id': 'user490208012', 'photo': 'photos/photo_32@12-04-2023_11-53-55.jpg', 'width': 1280, 'height': 960, 'text': '', 'text_entities': []}\n",
      "{'type': 'message', 'edited': '2023-04-12T13:09:53', 'edited_unixtime': '1681276193', 'from': 'Hanafi Haffidz', 'from_id': 'user44489326', 'photo': 'photos/photo_33@12-04-2023_12-06-37.jpg', 'width': 1280, 'height': 960, 'text': '', 'text_entities': []}\n"
     ]
    }
   ],
   "source": [
    "for row in reader.df[(reader.text_entities.apply(len) == 0)][\"messages\"]:\n",
    "    temp = copy.copy(row)\n",
    "    # Removing a few elements to make it easier to read\n",
    "    del temp[\"id\"]\n",
    "    del temp[\"date\"]\n",
    "    del temp[\"date_unixtime\"]\n",
    "    print(temp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of text entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text', 'type', 'user_id'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dict_keys(reader.text_entities, is_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bold',\n",
       " 'bot_command',\n",
       " 'code',\n",
       " 'email',\n",
       " 'link',\n",
       " 'mention',\n",
       " 'mention_name',\n",
       " 'plain',\n",
       " 'spoiler'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_entity_types(reader.text_entities, is_text_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "email\n",
      "{'type': 'plain', 'text': '(REF: CREATION OF AISG PROJECT/PACKAGE, LINKING WITH GITLAB)\\n\\n\\nFOR THOSE LOST AFTER THE DOCKER NOTEBOOK, LOOK AT THE README IN ASSIGNMENT 2. BASICALLY THE GIST OF IT IS AS FOLLOWS:\\n\\n# SET UP CRUFT\\n# CRUFT IS AN APP THAT AUTOMATES CREATION OF PROJECT FOLDERS BASED ON COOKIECUTTER TEMPLATES\\npip install cruft\\n\\n# RUN THIS TO BEGIN PROJECT CREATION WIZARD\\ncruft create '}\n",
      "{'type': 'link', 'text': 'https://github.com/aisingapore/ml-project-cookiecutter-gcp'}\n",
      "{'type': 'plain', 'text': \"\\n\\n# FILL IN AS FOLLOWS:\\nproject_name: AIAP DSP A2 MLOps\\ndescription: For AIAP 11's Deep Skilling Phase Assignment 2.\\nrepo_name: aiap-dsp-mlops\\nsrc_package_name: aiap_dsp_mlops\\nsrc_package_name_short: amlo\\ngcp_project_id: aiap-11-ds\\ngcr_personal_subdir: 1 - Yes\\nauthor_name: <YOUR_AIAP_EMAIL_WITHOUT_DOMAIN>\\nFor example, if your AIAP email is \"}\n",
      "{'type': 'email', 'text': 'chief_montgomery_scott@aiap.org'}\n",
      "{'type': 'plain', 'text': \" the value shall be chief_montgomery_scott.\\nopen_source_license: 3 - No license file\\n\\n# NOW YOU SHOULD HAVE A PROJECT FOLDER 'aiap-dsp-mlops'\\n# NOW WE WANT TO CONNECT TO AISG GITLAB. TO DO THAT, \\nVISIT \"}\n",
      "{'type': 'link', 'text': 'gitlab.aisingapore.net'}\n",
      "{'type': 'plain', 'text': ' TO CREATE A NEW PROJECT\\nCREATE New Project (ON THE RIGHT)\\nCREATE BLANK PROJECT\\n\\nPROJECT NAME: AIAP DSP MLOps\\nset \"Private\" for \"Visibility Level\"\\nUNCHECK: \"Initialize repository with a README\"\\nPROJECT URL TO YOUR OWN NAMESPACE i.e '}\n",
      "{'type': 'link', 'text': 'gitlab.aisingapore.net/'}\n",
      "{'type': 'plain', 'text': '<username>/ INSTEAD OF THE GROUP\\n\\n# ONCE CREATED, ADD MAINTAINERS:\\nBACK AT THE MAIN PAGE YOU SHOULD BE ABLE TO FIND THE PROJECT, ENTER IT.\\nPROJECT INFORMATION > MEMBERS > INVITE MEMBERS (ON THE RIGHT). ADD DEON, SYAKYR, KEVIN, AS MAINTAINERS'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'WHEN YOU FINISHED CREATING THE PROJECT ON GITLAB,\\nIT SHOULD HAVE GIVEN YOU SOME COMMANDS:\\n\\nGit global setup\\n\\ngit config --global '}\n",
      "{'type': 'link', 'text': 'user.name'}\n",
      "{'type': 'plain', 'text': ' \"Hanafi Haffidz\"\\ngit config --global '}\n",
      "{'type': 'link', 'text': 'user.email'}\n",
      "{'type': 'plain', 'text': ' \"'}\n",
      "{'type': 'email', 'text': 'hanafi_haffidz@aiap.sg'}\n",
      "{'type': 'plain', 'text': '\"\\n\\nCreate a new repository\\n\\ngit clone '}\n",
      "{'type': 'link', 'text': 'https://gitlab.aisingapore.net/hanafi_haffidz/aiap-dsp-mlops.git'}\n",
      "{'type': 'plain', 'text': '\\n\\n\\nRUN THE USERNAME AND EMAIL SETUP IF NECESSARY. BUT SHOULDN\\'T NEED TO AS YOU ALREADY BEEN PUSHING ASSIGNMENT 1\\n\\nCD into your local folder aiap-dsp-mlops, RUN THE COMMANDS gitlab gave under \"Push an existing folder\"'}\n",
      "----\n",
      "\n",
      "\n",
      "mention\n",
      "{'type': 'plain', 'text': 'hmm. '}\n",
      "{'type': 'mention', 'text': '@Wheynelau'}\n",
      "{'type': 'plain', 'text': ' , time to buy new laptop'}\n",
      "----\n",
      "{'type': 'mention', 'text': '@QZQ92'}\n",
      "{'type': 'plain', 'text': ' \\n'}\n",
      "{'type': 'link', 'text': 'https://towardsdatascience.com/deriving-backpropagation-with-cross-entropy-loss-d24811edeaf9'}\n",
      "{'type': 'plain', 'text': '\\n\\n'}\n",
      "{'type': 'link', 'text': 'https://www.mldawn.com/back-propagation-with-cross-entropy-and-softmax/'}\n",
      "{'type': 'plain', 'text': '\\n\\n'}\n",
      "{'type': 'link', 'text': 'https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entropy'}\n",
      "----\n",
      "{'type': 'mention', 'text': '@ElDopa'}\n",
      "{'type': 'plain', 'text': ' i am game for this'}\n",
      "----\n",
      "{'type': 'mention', 'text': '@QZQ92'}\n",
      "{'type': 'plain', 'text': ' '}\n",
      "{'type': 'link', 'text': 'https://medium.com/@ashwinnaidu1991/text-classification-with-transformers-70acaf65c4a4'}\n",
      "----\n",
      "{'type': 'mention', 'text': '@ElDopa'}\n",
      "{'type': 'plain', 'text': '  have you seen this? '}\n",
      "{'type': 'link', 'text': 'https://arxiv.org/abs/2304.03442'}\n",
      "----\n",
      "{'type': 'mention', 'text': '@username132123'}\n",
      "{'type': 'plain', 'text': ' i vote marvin for next journal presentation'}\n",
      "----\n",
      "{'type': 'mention', 'text': '@szhpullysystem'}\n",
      "{'type': 'plain', 'text': ' where u sia, i tot u came back before us'}\n",
      "----\n",
      "\n",
      "\n",
      "code\n",
      "{'type': 'plain', 'text': 'for >> 4. Specify the link to your private repository here, commit this file, and push to your remote branch: '}\n",
      "{'type': 'code', 'text': '<LINK_TO_REPOSITORY_HERE>'}\n",
      "----\n",
      "{'type': 'plain', 'text': \"03 March 2023 15:16:22 | 2023-03-03 07:16:22.146975: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\\n03 March 2023 15:16:22 | 2023-03-03 07:16:22.147011: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\\n03 March 2023 15:16:24 | /miniconda3/envs/aiap-dsp-mlops/lib/python3.8/site-packages/mlflow/types/schema.py:49: FutureWarning: In the future \"}\n",
      "{'type': 'code', 'text': 'np.object'}\n",
      "{'type': 'plain', 'text': ' will be defined as the corresponding NumPy scalar.'}\n",
      "----\n",
      "\n",
      "\n",
      "bot_command\n",
      "{'type': 'bot_command', 'text': '/start_roll_call'}\n",
      "----\n",
      "{'type': 'bot_command', 'text': '/in'}\n",
      "----\n",
      "{'type': 'bot_command', 'text': '/in@WhosInBot'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'I got the same issue yesterday cause I skipped this step: \"First, have VSCode open the repository that you have cloned previously by heading over to the top left hand corner, selecting File > Open Folder..., and entering the path to the repository. In this case, you should be navigating to the folder '}\n",
      "{'type': 'bot_command', 'text': '/polyaxon'}\n",
      "{'type': 'plain', 'text': '-v1-data/workspaces/<YOUR_NAME>/aiap-dsp-mlops\". After following through it, it works for me.'}\n",
      "----\n",
      "\n",
      "\n",
      "mention_name\n",
      "{'type': 'mention_name', 'text': 'Mayank', 'user_id': 565286338}\n",
      "{'type': 'plain', 'text': ' thats why I was saying the junior programmers are in trouble'}\n",
      "----\n",
      "{'type': 'mention_name', 'text': 'Mayank AIAP', 'user_id': 565286338}\n",
      "----\n",
      "{'type': 'mention_name', 'text': 'Mayank AIAP', 'user_id': 565286338}\n",
      "{'type': 'plain', 'text': ' : \\nso this color would work, but not without the unsqueze:\\nimg_pil = '}\n",
      "{'type': 'link', 'text': 'Image.open'}\n",
      "{'type': 'plain', 'text': '(imagefp)\\nimg_tensor = pil_to_tensor(img_pil)\\nprint(img_tensor.shape)\\n\\nedge_filter = torch.tensor(\\n    [[[[ 0,0,0],\\n       [ 0,1,0 ],\\n       [ 0,0,0]]]]).float()\\n\\nimg_tensor_ = F.conv2d(img_tensor.unsqueeze(1), edge_filter)\\nprint(img_tensor_.shape)\\n\\nimg_tensor_.clamp_min_(0).clamp_max_(1)\\nprint(img_tensor_.shape)\\n\\nimg = img_tensor_[:,0,:,:]\\ntensor_to_pil(img)'}\n",
      "----\n",
      "{'type': 'mention_name', 'text': 'Shu Ying', 'user_id': 5362962200}\n",
      "----\n",
      "{'type': 'mention_name', 'text': 'Mayank AIAP', 'user_id': 565286338}\n",
      "{'type': 'plain', 'text': ' @jia hao ah it was wordnet synset i was referring to'}\n",
      "----\n",
      "\n",
      "\n",
      "bold\n",
      "{'type': 'bold', 'text': 'Staying after 6 on Fri'}\n",
      "{'type': 'plain', 'text': '\\n\\n'}\n",
      "{'type': 'bold', 'text': 'Yes'}\n",
      "{'type': 'plain', 'text': ' (5ðŸ‘¥)\\nMayank\\nHan\\npaul\\nJF\\nZhi Xuan\\n\\n'}\n",
      "{'type': 'bold', 'text': 'No'}\n",
      "{'type': 'plain', 'text': ' (10ðŸ‘¥)\\nwer234wdf\\nAlvin\\nZhi Xuan\\nLoi\\nJong Ching\\nBryan\\nShu Ying\\nï½—ï½ï½Œï½”ï½…ï½’\\ntyl\\nZhi Qiang\\n\\nðŸ‘¥ 14 people responded'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'Useful Extensions in vscode:\\n'}\n",
      "{'type': 'bold', 'text': \"'autodocstring'\\n\"}\n",
      "{'type': 'plain', 'text': 'autofills a templated docstring when you type \"\"\" after \"def function(args) -> returntype:\"\\nin its config you can choose between google/numpy/sphinx style docstrings.\\n\\n'}\n",
      "{'type': 'bold', 'text': 'code formatters: \\n'}\n",
      "{'type': 'plain', 'text': 'not sure if options available as stock or i had additional extensions installed, if you go to Settings, in search bar type \"python formatting\", under the provider options, you can choose between autopep8, black, yapf. Read below for further differences and what your preferred formatter would be.\\n\\n'}\n",
      "{'type': 'link', 'text': 'https://blog.frank-mich.com/python-code-formatters-comparison-black-autopep8-and-yapf/'}\n",
      "{'type': 'plain', 'text': \"\\n\\nif autopep8 etc not available in your dropdown box then go search for it in Extensions and install.\\n\\nto turn it on, in Settings search bar type 'editor format'. you can choose to format on paste or format on save. then vscode will tidy the indents/whitespace/etc, wrap long lines etc.\"}\n",
      "----\n",
      "\n",
      "\n",
      "spoiler\n",
      "{'type': 'plain', 'text': \"In case you haven't had enough of transformers, here are transformers for \"}\n",
      "{'type': 'spoiler', 'text': 'TIME SERIES'}\n",
      "{'type': 'plain', 'text': '\\n'}\n",
      "{'type': 'link', 'text': 'https://medium.com/mlearning-ai/transformer-implementation-for-time-series-forecasting-a9db2db5c820'}\n",
      "{'type': 'plain', 'text': '\\n'}\n",
      "{'type': 'link', 'text': 'https://towardsdatascience.com/advances-in-deep-learning-for-time-series-forecasting-and-classification-winter-2023-edition-6617c203c1d1'}\n",
      "----\n",
      "\n",
      "\n",
      "link\n",
      "{'type': 'plain', 'text': 'i found this read earlier for making a random forest from scratch, i found it to be a good read\\n'}\n",
      "{'type': 'link', 'text': 'https://carbonati.github.io/posts/random-forests-from-scratch/'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://neptune.ai/blog/cross-validation-mistakes#:~:text=It%20is%20a%20major%20problem,it%20would%20have%20not%20known'}\n",
      "{'type': 'plain', 'text': '.'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://www.finews.asia/people/28933-grab-nabs-ex-gic-chief-economist'}\n",
      "{'type': 'plain', 'text': \" this was what i was sharing with some of u guys earlier. grab nabs this guy, then aisg nabbed him from grab haha. he's the head of the product team at lvl 5\"}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://drivendata.github.io/cookiecutter-data-science/'}\n",
      "{'type': 'plain', 'text': '\\n\\n'}\n",
      "{'type': 'link', 'text': 'https://huggingface.co/docs/transformers/model_doc/bert'}\n",
      "{'type': 'plain', 'text': '\\n\\n'}\n",
      "{'type': 'link', 'text': 'https://polyaxon.com/'}\n",
      "{'type': 'plain', 'text': '\\n\\n'}\n",
      "{'type': 'link', 'text': 'https://ml-ops.org/content/end-to-end-ml-workflow'}\n",
      "----\n",
      "{'type': 'plain', 'text': \"Did you know that there's a particle accelerator just behind our building? \"}\n",
      "{'type': 'link', 'text': 'https://ssls.nus.edu.sg/'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'here you go: '}\n",
      "{'type': 'link', 'text': 'https://discord.gg/CcFhmVe3'}\n",
      "{'type': 'plain', 'text': '\\n\\nit was created by JF'}\n",
      "----\n",
      "{'type': 'plain', 'text': \"Yup, I started that Discord server. You can use this link if Walter's link doesn't work: \"}\n",
      "{'type': 'link', 'text': 'https://discord.gg/zuNUDB5TTM'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'Useful Extensions in vscode:\\n'}\n",
      "{'type': 'bold', 'text': \"'autodocstring'\\n\"}\n",
      "{'type': 'plain', 'text': 'autofills a templated docstring when you type \"\"\" after \"def function(args) -> returntype:\"\\nin its config you can choose between google/numpy/sphinx style docstrings.\\n\\n'}\n",
      "{'type': 'bold', 'text': 'code formatters: \\n'}\n",
      "{'type': 'plain', 'text': 'not sure if options available as stock or i had additional extensions installed, if you go to Settings, in search bar type \"python formatting\", under the provider options, you can choose between autopep8, black, yapf. Read below for further differences and what your preferred formatter would be.\\n\\n'}\n",
      "{'type': 'link', 'text': 'https://blog.frank-mich.com/python-code-formatters-comparison-black-autopep8-and-yapf/'}\n",
      "{'type': 'plain', 'text': \"\\n\\nif autopep8 etc not available in your dropdown box then go search for it in Extensions and install.\\n\\nto turn it on, in Settings search bar type 'editor format'. you can choose to format on paste or format on save. then vscode will tidy the indents/whitespace/etc, wrap long lines etc.\"}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://gist.github.com/joshbuchea/6f47e86d2510bce28f8e7f42ae84c716'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://meet.google.com/pxu-fhkx-fmn'}\n",
      "{'type': 'plain', 'text': '\\nMLops link'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'git remote -v\\nnew folder\\ngit init\\ngit remote add origin '}\n",
      "{'type': 'link', 'text': 'https://gitlab.aisingapore.net/'}\n",
      "{'type': 'plain', 'text': '<aiap_name>/aiap-dsp-mlops'}\n",
      "----\n",
      "{'type': 'plain', 'text': '(REF: CREATION OF AISG PROJECT/PACKAGE, LINKING WITH GITLAB)\\n\\n\\nFOR THOSE LOST AFTER THE DOCKER NOTEBOOK, LOOK AT THE README IN ASSIGNMENT 2. BASICALLY THE GIST OF IT IS AS FOLLOWS:\\n\\n# SET UP CRUFT\\n# CRUFT IS AN APP THAT AUTOMATES CREATION OF PROJECT FOLDERS BASED ON COOKIECUTTER TEMPLATES\\npip install cruft\\n\\n# RUN THIS TO BEGIN PROJECT CREATION WIZARD\\ncruft create '}\n",
      "{'type': 'link', 'text': 'https://github.com/aisingapore/ml-project-cookiecutter-gcp'}\n",
      "{'type': 'plain', 'text': \"\\n\\n# FILL IN AS FOLLOWS:\\nproject_name: AIAP DSP A2 MLOps\\ndescription: For AIAP 11's Deep Skilling Phase Assignment 2.\\nrepo_name: aiap-dsp-mlops\\nsrc_package_name: aiap_dsp_mlops\\nsrc_package_name_short: amlo\\ngcp_project_id: aiap-11-ds\\ngcr_personal_subdir: 1 - Yes\\nauthor_name: <YOUR_AIAP_EMAIL_WITHOUT_DOMAIN>\\nFor example, if your AIAP email is \"}\n",
      "{'type': 'email', 'text': 'chief_montgomery_scott@aiap.org'}\n",
      "{'type': 'plain', 'text': \" the value shall be chief_montgomery_scott.\\nopen_source_license: 3 - No license file\\n\\n# NOW YOU SHOULD HAVE A PROJECT FOLDER 'aiap-dsp-mlops'\\n# NOW WE WANT TO CONNECT TO AISG GITLAB. TO DO THAT, \\nVISIT \"}\n",
      "{'type': 'link', 'text': 'gitlab.aisingapore.net'}\n",
      "{'type': 'plain', 'text': ' TO CREATE A NEW PROJECT\\nCREATE New Project (ON THE RIGHT)\\nCREATE BLANK PROJECT\\n\\nPROJECT NAME: AIAP DSP MLOps\\nset \"Private\" for \"Visibility Level\"\\nUNCHECK: \"Initialize repository with a README\"\\nPROJECT URL TO YOUR OWN NAMESPACE i.e '}\n",
      "{'type': 'link', 'text': 'gitlab.aisingapore.net/'}\n",
      "{'type': 'plain', 'text': '<username>/ INSTEAD OF THE GROUP\\n\\n# ONCE CREATED, ADD MAINTAINERS:\\nBACK AT THE MAIN PAGE YOU SHOULD BE ABLE TO FIND THE PROJECT, ENTER IT.\\nPROJECT INFORMATION > MEMBERS > INVITE MEMBERS (ON THE RIGHT). ADD DEON, SYAKYR, KEVIN, AS MAINTAINERS'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'WHEN YOU FINISHED CREATING THE PROJECT ON GITLAB,\\nIT SHOULD HAVE GIVEN YOU SOME COMMANDS:\\n\\nGit global setup\\n\\ngit config --global '}\n",
      "{'type': 'link', 'text': 'user.name'}\n",
      "{'type': 'plain', 'text': ' \"Hanafi Haffidz\"\\ngit config --global '}\n",
      "{'type': 'link', 'text': 'user.email'}\n",
      "{'type': 'plain', 'text': ' \"'}\n",
      "{'type': 'email', 'text': 'hanafi_haffidz@aiap.sg'}\n",
      "{'type': 'plain', 'text': '\"\\n\\nCreate a new repository\\n\\ngit clone '}\n",
      "{'type': 'link', 'text': 'https://gitlab.aisingapore.net/hanafi_haffidz/aiap-dsp-mlops.git'}\n",
      "{'type': 'plain', 'text': '\\n\\n\\nRUN THE USERNAME AND EMAIL SETUP IF NECESSARY. BUT SHOULDN\\'T NEED TO AS YOU ALREADY BEEN PUSHING ASSIGNMENT 1\\n\\nCD into your local folder aiap-dsp-mlops, RUN THE COMMANDS gitlab gave under \"Push an existing folder\"'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'btw for the mac and mini conda\\nsaw this git ticket in 2022 jul \\n'}\n",
      "{'type': 'link', 'text': 'https://github.com/microsoft/recommenders/issues/1762'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'New whatsapp group:\\n'}\n",
      "{'type': 'link', 'text': 'https://chat.whatsapp.com/JHqBZWsA1htItQnyTNOoAk'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://aisingapore.github.io/ml-project-cookiecutter-gcp/guide-for-user/06-data-storage-versioning/'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://twitter.com/tobyordoxford/status/1627418366137737216?s=61&t=Lui5mLTWATYvBK1nIccuTA'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://huyenchip.com/'}\n",
      "{'type': 'plain', 'text': ' Suggested resource creator for MLOps (suggested by Deon)'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://www.straitstimes.com/singapore/three-people-taken-to-hospital-after-accident-involving-nus-shuttle-bus-and-sbs-transit-bus'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://stackoverflow.com/questions/2517190/how-do-i-force-git-to-use-lf-instead-of-crlf-under-windows'}\n",
      "{'type': 'plain', 'text': ' for those in windows CRLF hell'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://stackoverflow.com/questions/70946140/docker-desktop-wsl-ext4-vhdx-too-large'}\n",
      "{'type': 'plain', 'text': ' for those whose docker ate your drive space'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'FYI someone from my Discord server did a ChatGPT Free App. You can check it out here. Source code is available. '}\n",
      "{'type': 'link', 'text': 'https://freechatgpt.chat/'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'The next ML Singapore meetup is happening on Mon 13 March. Sign up here: '}\n",
      "{'type': 'link', 'text': 'https://www.meetup.com/machine-learning-singapore/events/291996393/'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'reminder to fill up: Hi Batch 12, we are having a MLOps Q&A session later in the afternoon, please fill in this form '}\n",
      "{'type': 'link', 'text': 'https://docs.google.com/forms/d/e/1FAIpQLSe91ca6LtT2kTfQuEOVHFoZNxBli-qyxPb9yINrLbsg4ZOJyA/viewform?usp=sharing'}\n",
      "{'type': 'plain', 'text': ' for the MLOps team to better facilitate the session. Thank you!'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'Mayank, you can join the Q&A now on Google Meet: '}\n",
      "{'type': 'link', 'text': 'https://meet.google.com/pxu-fhkx-fmn'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'reminded me i had this saved '}\n",
      "{'type': 'link', 'text': 'https://huyenchip.com/ml-interviews-book/'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://www.tensorflow.org/tutorials/structured_data/imbalanced_data'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'For those who wanna download books or see research papers for free: \\n1. Papers visit: '}\n",
      "{'type': 'link', 'text': 'https://sci-hub.ru/'}\n",
      "{'type': 'plain', 'text': '\\n2. Books visit: '}\n",
      "{'type': 'link', 'text': 'https://libgen.is/'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://www.youtube.com/watch?v=tpCFfeUEGs8'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'Optimizing Model Parameters â€” PyTorch Tutorials 1.13.1+cu117 documentation\\n'}\n",
      "{'type': 'link', 'text': 'https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html'}\n",
      "{'type': 'plain', 'text': ''}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://youtu.be/khUVIZ3MON8'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://www.healthxchange.sg/food-nutrition/food-tips/local-breakfast-choices-is-dim-sum-healthy'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://www.youtube.com/watch?v=Ilg3gGewQ5U'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://www.youtube.com/watch?v=mq-JXcYOQHc'}\n",
      "----\n",
      "{'type': 'plain', 'text': '\"Mr Ryan San, CEO of '}\n",
      "{'type': 'link', 'text': 'EM2.AI'}\n",
      "{'type': 'plain', 'text': ' (under Q&M Dental Group) shared about the 100E project which developed an AI model to automate dental charting and help dentists detect diseases from dental X-rays as well as recommend a treatment plan for the patient.\"'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'If anyone enjoys puzzles, Harvardâ€™s CS50x Puzzle Day 2023 is open for registration: '}\n",
      "{'type': 'link', 'text': 'https://cs50.harvard.edu/x/2023/puzzles/'}\n",
      "{'type': 'plain', 'text': '\\n\\n2022â€™s for reference: '}\n",
      "{'type': 'link', 'text': 'https://cdn.cs50.net/2022/x/events/puzzles/puzzles.pdf'}\n",
      "{'type': 'plain', 'text': ''}\n",
      "----\n",
      "{'type': 'mention', 'text': '@QZQ92'}\n",
      "{'type': 'plain', 'text': ' \\n'}\n",
      "{'type': 'link', 'text': 'https://towardsdatascience.com/deriving-backpropagation-with-cross-entropy-loss-d24811edeaf9'}\n",
      "{'type': 'plain', 'text': '\\n\\n'}\n",
      "{'type': 'link', 'text': 'https://www.mldawn.com/back-propagation-with-cross-entropy-and-softmax/'}\n",
      "{'type': 'plain', 'text': '\\n\\n'}\n",
      "{'type': 'link', 'text': 'https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entropy'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://hydra.cc/docs/1.1/plugins/optuna_sweeper/'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://twitter.com/grdecter/status/1635996439171018757?s=12&t=oTzRSyzb769T8nS7_siz2Q'}\n",
      "----\n",
      "{'type': 'mention_name', 'text': 'Mayank AIAP', 'user_id': 565286338}\n",
      "{'type': 'plain', 'text': ' : \\nso this color would work, but not without the unsqueze:\\nimg_pil = '}\n",
      "{'type': 'link', 'text': 'Image.open'}\n",
      "{'type': 'plain', 'text': '(imagefp)\\nimg_tensor = pil_to_tensor(img_pil)\\nprint(img_tensor.shape)\\n\\nedge_filter = torch.tensor(\\n    [[[[ 0,0,0],\\n       [ 0,1,0 ],\\n       [ 0,0,0]]]]).float()\\n\\nimg_tensor_ = F.conv2d(img_tensor.unsqueeze(1), edge_filter)\\nprint(img_tensor_.shape)\\n\\nimg_tensor_.clamp_min_(0).clamp_max_(1)\\nprint(img_tensor_.shape)\\n\\nimg = img_tensor_[:,0,:,:]\\ntensor_to_pil(img)'}\n",
      "----\n",
      "{'type': 'plain', 'text': \"if anyone came across the concept of upsampling via convolution for image segmentation (e.g. in U-Net), and was confused about how it works, this is the best explanation I've found by far: \"}\n",
      "{'type': 'link', 'text': 'https://www.youtube.com/watch?v=pQYUGaFMsGU&ab_channel=IITKharagpurJuly2018'}\n",
      "{'type': 'plain', 'text': '. Starts around 14 mins in but worth watching the whole thing through'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'this is also pretty good to explain deconvolution in the 2d case (which is more common) '}\n",
      "{'type': 'link', 'text': 'https://www.youtube.com/watch?v=ilkSwsggSNM&list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51&index=138&ab_channel=SebastianRaschka'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'Take note, A4P2, 5.3 load data using tf.keras.preprocessing.image.ImageDataGenerator is deprecated. It still works as of tf 2.11.0 but you might wish to use '}\n",
      "{'type': 'link', 'text': 'https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory'}\n",
      "{'type': 'plain', 'text': ' instead.'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://www.tensorflow.org/tutorials/load_data/images'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://hacksingapore.com/'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://youtu.be/mL5wI3tkXkw'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'full 20 minute interview if you have time '}\n",
      "{'type': 'link', 'text': 'https://m.youtube.com/watch?v=540vzMlf-54&t=398s'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'Those with non nvidia GPUs may wish to look into this: '}\n",
      "{'type': 'link', 'text': 'https://github.com/microsoft/directml'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'I quite like this article '}\n",
      "{'type': 'link', 'text': 'https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'liong said this was helpful for him \\n'}\n",
      "{'type': 'link', 'text': 'https://keras.io/examples/vision/visualizing_what_convnets_learn/'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'Just found this for gradient ascent:\\n'}\n",
      "{'type': 'link', 'text': 'https://www.coursera.org/learn/visualizing-filters-cnn-tensorflow/home/week/1'}\n",
      "{'type': 'plain', 'text': \"\\n(if you can't access, essentially its just these:)\\n\"}\n",
      "{'type': 'link', 'text': 'https://production-video-recordings-skillspace.gp.coursera.org/ch_vid-en-US-8c43cfab-1f07-4487-8628-ee3dedf84c30-f333dd73-f0c1-4eb5-92fc-1989d5ed0b76-piZOvyKD-20201205173154.mp4'}\n",
      "{'type': 'plain', 'text': '\\n'}\n",
      "{'type': 'link', 'text': 'https://colab.research.google.com/drive/1RSwWYlYxMOday0WvnUt2P177Auqk4IgK'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://poloclub.github.io/cnn-explainer/'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://tensorspace.org/'}\n",
      "{'type': 'plain', 'text': '\\n\\nThis is also quite cool'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://www.youtube.com/watch?v=brYtVpa5QBo'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'This entire playlist visualises the different fundamentals of CNN, stride, padding and why certain kernel size is used. \\n\\n'}\n",
      "{'type': 'link', 'text': 'https://youtube.com/playlist?list=PLZDCDMGmelH-pHt-Ij0nImVrOmj8DYKbB'}\n",
      "----\n",
      "{'type': 'plain', 'text': '2 methods to strip out HTML tags from a string, one using regex and one just using Python standard library:\\n'}\n",
      "{'type': 'link', 'text': 'https://tutorialedge.net/python/removing-html-from-string/'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://github.com/PaulSZH95/data_collection_for_job_application'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'from: '}\n",
      "{'type': 'link', 'text': 'https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://pypi.org/project/contractions/'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://towardsdatascience.com/using-shap-values-to-explain-how-your-machine-learning-model-works-732b3f40e137'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'this helped\\n'}\n",
      "{'type': 'link', 'text': 'https://blog.floydhub.com/attention-mechanism/'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/linear_models/Sentiment%20Analysis%20with%20Logistic%20Regression.html'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://www.youtube.com/watch?v=XXtpJxZBa2c'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'ChatGPT Gets Its â€œWolfram Superpowersâ€!â€”Stephen Wolfram Writings\\n'}\n",
      "{'type': 'link', 'text': 'https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/'}\n",
      "{'type': 'plain', 'text': ''}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://blog.floydhub.com/attention-mechanism/#:~:text=In%20Luong%20Attention%2C%20there%20are,to%20calculate%20the%20alignment%20scores'}\n",
      "{'type': 'plain', 'text': '.'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'earlier slides: '}\n",
      "{'type': 'link', 'text': 'https://docs.google.com/presentation/d/1npiCzLZ3DZJ_oE0wuF6XfITnX_fR7C1BIWwjgzj61vY/edit?usp=sharing'}\n",
      "{'type': 'plain', 'text': '\\n\\nrecording: '}\n",
      "{'type': 'link', 'text': 'https://drive.google.com/file/d/1CLgUfTcjV2lyZtV2rlM0piXVn_nwo_HU/view?usp=sharing'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'NEW LINK: '}\n",
      "{'type': 'link', 'text': 'https://meet.google.com/jcz-sacg-qhc'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://futureoflife.org/open-letter/pause-giant-ai-experiments/'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'KLASS \\n'}\n",
      "{'type': 'link', 'text': 'https://forms.gle/2tg4coHPWhU4g7Cc6'}\n",
      "{'type': 'plain', 'text': ' \\n\\nBIPO \\n'}\n",
      "{'type': 'link', 'text': 'https://forms.gle/CiiKp46Sh1CgQXhG8'}\n",
      "{'type': 'plain', 'text': ' \\n\\nAI PRODUCTS \\n'}\n",
      "{'type': 'link', 'text': 'http://shorturl.at/mqtBU'}\n",
      "{'type': 'plain', 'text': ' \\n\\nBMYLLM \\n'}\n",
      "{'type': 'link', 'text': 'https://docs.google.com/forms/d/1VgMsxK6nHBYlTVktUFVUWq1OfoIEwmifR6Fxr8Oz28s/viewform?edit_requested=true'}\n",
      "{'type': 'plain', 'text': ' \\n\\nAI LEARNING PLATFORM '}\n",
      "{'type': 'link', 'text': 'https://docs.google.com/forms/d/1iuF5MPwa9lITNJJ21PeFHbWsAaX7wMj7FS1sNCbJlIA/viewform?edit_requested=true'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://meet.google.com/pcj-wpqo-fps'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'Video call link: '}\n",
      "{'type': 'link', 'text': 'https://meet.google.com/gui-yuhp-gdt'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://web.stanford.edu/~jurafsky/slp3/'}\n",
      "{'type': 'plain', 'text': \" recommended book for NLP by a stanford prof, in the first few paras there's a pdf version available as well.\"}\n",
      "----\n",
      "{'type': 'mention', 'text': '@QZQ92'}\n",
      "{'type': 'plain', 'text': ' '}\n",
      "{'type': 'link', 'text': 'https://medium.com/@ashwinnaidu1991/text-classification-with-transformers-70acaf65c4a4'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'Upcoming Jax/Diffusers community sprint for the Generative AI enthusiasts: '}\n",
      "{'type': 'link', 'text': 'https://github.com/huggingface/community-events/tree/main/jax-controlnet-sprint'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'Wes McKinney, creator of pandas, has made his text book \"Python for Data Analysis\" available for free. He has a chapter on time series: '}\n",
      "{'type': 'link', 'text': 'https://wesmckinney.com/book/time-series.html'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'How to handle time series data with ease\\n'}\n",
      "{'type': 'link', 'text': 'https://pandas.pydata.org/docs/getting_started/intro_tutorials/09_timeseries.html'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'Friends, a few weeks before AIAP, I caught up with an old friend who is the regional CTO of Kyndryl, an IBM spinoff which employs 90,000 people around the world.\\n'}\n",
      "{'type': 'link', 'text': 'https://en.wikipedia.org/wiki/Kyndryl'}\n",
      "{'type': 'plain', 'text': \"\\nKyndryl is the worldâ€™s largest IT infrastructure services provider and is hungry for AI engineers.\\nMy friend told me to look him up when I graduate. If anyone is interested, let me know and I'll be happy to introduce.\"}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://www.reddit.com/r/okbuddyphd/comments/12akks9/stop_doing_nlp/?utm_source=share&utm_medium=ios_app&utm_name=iossmf&utm_content=2&utm_term=15'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'This sub has some great AI memes lol '}\n",
      "{'type': 'link', 'text': 'https://www.reddit.com/r/okbuddyphd/comments/127lmt9/me_secretly_sprinkling_a_little_training_data/?utm_source=share&utm_medium=ios_app&utm_name=ioscss&utm_content=1&utm_term=1'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'Video version: Sparks of AGI: early experiments with GPT-4\\n'}\n",
      "{'type': 'link', 'text': 'https://www.youtube.com/watch?v=qbIk7-JPB2c'}\n",
      "----\n",
      "{'type': 'plain', 'text': 'Yann LeCun and Andrew Ng: Why the 6-month AI Pause is a Bad Idea\\n'}\n",
      "{'type': 'link', 'text': 'https://www.youtube.com/watch?v=BY9KV8uCtj4'}\n",
      "----\n",
      "{'type': 'mention', 'text': '@ElDopa'}\n",
      "{'type': 'plain', 'text': '  have you seen this? '}\n",
      "{'type': 'link', 'text': 'https://arxiv.org/abs/2304.03442'}\n",
      "----\n",
      "{'type': 'plain', 'text': \"In case you haven't had enough of transformers, here are transformers for \"}\n",
      "{'type': 'spoiler', 'text': 'TIME SERIES'}\n",
      "{'type': 'plain', 'text': '\\n'}\n",
      "{'type': 'link', 'text': 'https://medium.com/mlearning-ai/transformer-implementation-for-time-series-forecasting-a9db2db5c820'}\n",
      "{'type': 'plain', 'text': '\\n'}\n",
      "{'type': 'link', 'text': 'https://towardsdatascience.com/advances-in-deep-learning-for-time-series-forecasting-and-classification-winter-2023-edition-6617c203c1d1'}\n",
      "----\n",
      "{'type': 'link', 'text': 'https://docs.google.com/spreadsheets/d/1VAKQILumGQ_Ex5he47y4IF6XW9ZpfavYUq5mPORz-lg/edit#gid=0'}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "def get_text_entity_type_examples(\n",
    "    text_entities: pd.Series, which_type: str, to_print: bool = True\n",
    ") -> pd.Series:\n",
    "    \"\"\"Display text entities which contain an element of a certain type.\"\"\"\n",
    "\n",
    "    def contains_type(text_entities, which_type):\n",
    "        \"Filtering function. See if any of the text entities in message are of desired type\"\n",
    "        return any(\n",
    "            text_entity.get(\"type\") == which_type for text_entity in text_entities\n",
    "        )\n",
    "\n",
    "    # Filter messages using filter function\n",
    "    filtered_text_entities = text_entities[\n",
    "        text_entities.apply(contains_type, which_type=which_type)\n",
    "    ]\n",
    "    # Prettily print the messages containing the desired type\n",
    "    if to_print:\n",
    "        for text_entity_row in filtered_text_entities:\n",
    "            for text_entity in text_entity_row:\n",
    "                print(text_entity)\n",
    "            print(\"----\")\n",
    "    # Get the text entities and return\n",
    "    return filtered_text_entities\n",
    "\n",
    "\n",
    "# Print out text entities of each type (other than plain, which has too many entries)\n",
    "for entity_type in get_text_entity_types(reader.text_entities):\n",
    "    if entity_type != \"plain\":\n",
    "        print(\"\\n\\n\" + entity_type)\n",
    "        get_text_entity_type_examples(reader.text_entities, entity_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple, will just add the text from all the entity types into the message text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-21T15:27:19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-21T16:04:03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-21T17:15:25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-21T17:15:35</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-21T17:16:36</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>2023-04-12T11:55:02</td>\n",
       "      <td>Hanafi Haffidz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>2023-04-12T11:57:27</td>\n",
       "      <td>Wayne Lau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>2023-04-12T11:58:00</td>\n",
       "      <td>paul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>2023-04-12T12:04:39</td>\n",
       "      <td>Wayne Lau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>2023-04-12T12:06:37</td>\n",
       "      <td>Hanafi Haffidz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>350 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date            from\n",
       "0    2023-02-21T15:27:19             NaN\n",
       "1    2023-02-21T16:04:03             NaN\n",
       "2    2023-02-21T17:15:25             NaN\n",
       "3    2023-02-21T17:15:35             NaN\n",
       "4    2023-02-21T17:16:36             NaN\n",
       "..                   ...             ...\n",
       "345  2023-04-12T11:55:02  Hanafi Haffidz\n",
       "346  2023-04-12T11:57:27       Wayne Lau\n",
       "347  2023-04-12T11:58:00            paul\n",
       "348  2023-04-12T12:04:39       Wayne Lau\n",
       "349  2023-04-12T12:06:37  Hanafi Haffidz\n",
       "\n",
       "[350 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check messages with just a full stop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These no longer exist because I've now incorporated all the types of text into the plaintext. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TelegramReader' object has no attribute 'plaintext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reader\u001b[39m.\u001b[39;49mplaintext[\u001b[39m\"\u001b[39m\u001b[39mplaintext\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlen\u001b[39m)\u001b[39m.\u001b[39mvalue_counts()\u001b[39m.\u001b[39msort_index()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TelegramReader' object has no attribute 'plaintext'"
     ]
    }
   ],
   "source": [
    "reader.plaintext[\"plaintext\"].apply(len).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plaintext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Okie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Thx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>/in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>thx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Thx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>haha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>ðŸ˜ˆ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>LOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Yea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    plaintext\n",
       "5        Okie\n",
       "6         Thx\n",
       "24        /in\n",
       "78        thx\n",
       "84        Thx\n",
       "126      haha\n",
       "158         ðŸ˜ˆ\n",
       "249       LOL\n",
       "280       Yea"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.plaintext[reader.plaintext[\"plaintext\"].apply(len) < 5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
